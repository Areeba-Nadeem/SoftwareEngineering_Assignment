# SoftwareEngineering_Assignment

Paper#1:
Title:
        From Ad-Hoc Data Analytics to DataOps

Author: 
              Aiswarya Munappy, David Issa Mattos, Jan Bosch, Helena Holmström Olsson, Anas Dakkak

Conference name:
                                   International Conference on Software and Systems Process
Introduction And Motivation:

Data is the key asset for the organizations as it helps in better decision making, analyses performance and solving problems, to analyses the consumer behavior and market and so on. Moreover, data is the backbone for many hot and trending technologies like machine learning and deep learning .Increased importance of data lead to the acquisition and storage of data in higher volumes which in turn gave rise to fields like Big Data, data mining and data warehousing. Due to the operations initiated by the engineers or by the change in data sources, continuous data changes happen and there is the requirement for data versioning and sharing techniques. Thus, data management becomes vital for all organizations to collect, store, organize, protect, verify, and process essential data. Data being the fuel for digital economy need for data products like machine learning datasets, dashboards and visualizations is tremendously increasing. Organizations invest in data science and data analytics to solve problems with the collected data. Organizations realize that data is the key factor of success and as a result they invest an enormous amount of money in the development of data products. Data products are built through a sequence of steps called data life cycle wherein for each step there will be both hardware and software requirements. Due to this reason, it is very essential to find the right balance of investment on requirements in different stages of data life cycle. Data management, data life cycle management, data pipeline robustness, fast delivery of high quality insights are some of the major data problems that prevents companies to achieve their full potential. Dev Ops is a methodology adapted in Software Engineering to aid agile software development. Agile methodology focuses on empowering individuals, rapid production of working software, close collaboration with customers and quick response to the change in customer requirements. Agile development is directly facilitated by CI/CD practices because it aids in software changes reaching production more frequently and rapidly. Consequently, customers get more opportunities to experience and provide feedback on changes. Industries apply agile methodology, Data Ops and CI/CD methodologies in the software development.

Research Methodology:
The goals of this study was to formulate a definition for Data Ops and to identify the phases of Data Ops evolution. 
A. Setting the RQs The RQs defined in the study are as given below:- 
• RQ1. How do practitioners define “Data Ops”? The goal of this research question is to achieve an aligned understanding on the concept of Data Ops, aiding communication among researchers and practitioners when presenting results related to Data Ops.
 • RQ2. What are the different maturity stages Ericsson has gone through while trying to evolve from ad-hoc data analysis to Data Ops? This RQ seeks to identify data strategies that are used at Ericsson to do Data Analytics in industrial systems and also the drivers for adoption at each stage to move to the successive stages. overlap between the two data analytic approaches and the practical difficulties Ericsson encounter while trying to completely adopt Data Ops as their analytic approach 
C. Need for MLR: 
To learn more about the concept of Data Ops, we did an initial search for the formal academic literature in different databases such as Google Scholar, IEEE Explore, ACM digital library, Web of Science, Scopus and Science Direct. However, we could not find a considerable number of peer reviewed papers on the topic. Consequently, we decided to conduct a Multi vocal Literature Review, based on all available literature on a topic. According to Ogawa et.al a broader view about a particular topic can be obtained by using this wide spectrum of literature as they include the voices and opinions of academics, practitioners, independent researchers, development firms, and others who have experience on the topic. Grouse et al. states that the practitioners produce literature based on their experience, but most of them are not published as academic literature. Also, voice of the practitioners better reflect the important current state-of-the-art practice in SE. Therefore, it is important to include Grey literature too in the systematic review 
D. Process of MLR:  The Multi-vocal literature review procedure adopted for the study is demonstrated in Fig. 1. The systematic review employs string-based database search to select relevant studies from the literature. All retrieved literature was exported to MS Excel for further processing. The exported references were screened based on inclusion exclusion criteria. The inclusion and exclusion criteria considered in our study are as shown below. 
• Inclusion Criteria:
(1) Papers and Google links describing the steps of Data Ops approach, essential components of Data Ops, benefits and challenges. 
(2) Papers describing the Bigdata pipelines, Big data processing pipelines 
• Exclusion Criteria: (1) Duplicates and non-English 

E. Exploratory Case study:  The study was conducted in collaboration with Ericsson. Ericsson is a Swedish multinational network and telecommunications company. The company provides services, software and infrastructure in information and communications technology. The objective of the study is to explore the essential stages of Data Analytic approach which Ericsson follows in their real-world settings and also to investigate its similarity to the popular Data Ops approach. Each case in the study refers to a team at Ericsson working with the data they collect from different sources. For the study, a sample pool of Data Fig. 1. Multi-vocal literature review procedure applied in the study Scientists, Data Analysts and Data Engineers were selected by one of the authors according to their expertise in the area of Data Analytics. Request to participate in the study was sent and after the interviews, interviewees suggested some of their colleagues and all together 10 interviews were conducted for this study. Table 1 illustrates the role of our interviewees and the use cases. Data Scientist team R7 Data Scientist H Building data pipelines for CI and CD data R8 Program Manager 

F. Data Collection: 
Empirical data was collected through semi-structured interviews were used to acquire qualitative data. Based on the objective of research to explore data analytic approach employed at Ericsson, an interview guide with 45 questions categorized into six sections was formulated. The first and second sections concentrated on the background of the interviewee. The third and fourth sections focused on the data collection and processing in various use-cases and the last section inquired in detail about data testing and monitoring practices and the impediments faced during every phase of the data pipeline. The interview guide was prepared by the firs Exploratory Case study t author and was reviewed by all the other authors. Based on the comments and recommendations some additional questions were added, a few similar questions were merged together and some irrelevant questions were removed finally forming an interview protocol with 30 questions spread across six different categories. All interviews were conducted via video conferencing except for three which were done face-to-face and each interview lasted 50 to 100 minutes. 

G. Data Analysis: After the interviews, audio recordings of interview were sent for transcription and a summary of each interview was prepared by the first author highlighting the important focus points of the interview. The investigated points from the summary were cross-checked several times with the audio recordings and interview transcripts obtained after transcription. A theoretical thematic data analysis approach was selected for coding . The first author coded each relevant segment of the interview transcript in Vivo. For the first iteration, the objective was to identify the use-cases discussed by each interviewee and phases of data analytics used by their team. After identifying the phases, a second iteration was performed to investigate the impediments encountered to completely set up Data Ops practices at Ericsson. Thematic coding was performed by setting high level themes as (i) Data Collection, (ii) Data Analytics (iii) Data Ops (iv) Automation (v) Data Testing, (vi) Data monitoring, (vii) Agile environment. After careful analysis of collected data, the first two authors agreed on the presentation of results in the paper. From the analysis, results were tabulated and sent to the other authors for their reflections and then the final summary of the cases and results were sent to the interviewees for validating the inferred results.
Conclusion:
The collection of high quality data gives companies a significant competitive advantage in their decision making process. It helps in understanding customer behavior and enables the use and deployment of new technologies based on machine learning.
Paper#2:
-----------------
Title:
          Digital Re-imagination of Software and Systems Processes for Quality Engineering: iSPIN Approach

Author:
          Padmalata Nistala, Asha Rajbhoj, Vinay Kulkarni, Kesav Vithal Nori

Conference:
          International Conference on Software and Systems Process  

Introduction And Motivation:
 Software quality has become the lever of differentiation in today’s competitive marketplace. Quality at speed is the customer demand and automation is the biggest bottleneck holding the evolution of quality function. Increased levels of automation and intelligence in software engineering are the emerging trends across the IT field. As systems and software processes guide the life cycle activities and are the vehicles for building quality, it is necessary to look at the process infrastructure for the extent of process automation support provided and the digital enablement. This paper maps out the existing process infrastructure support in industry practice and proposes a roadmap for digital re-imagination of software and systems processes. Harmonizing the quality engineering themes with digital technologies, we propose a framework for building an intelligent software process infrastructure, iSPIN that can help in digital re-imagination of software and systems lifecycle processes. The framework has been implemented using digital technologies and has been piloted with one of the industry business unit for re-imagination of “proposal process”. The proposed iSPIN framework will help in unprecedented automation and quality engineering at each process step and paves the way towards realizing the dictums of “Quality at Speed” and “Digital transformation of Software Process”.
 
Research Methodology:
Increased levels of automation and intelligence in software engineering mostly for all intents and purposes specifically are the emerging trends across the IT field, which kind of basically kind of shows that quality at speed really particularly is the customer demand and automation really mostly really essentially is the very fairly sort of sort of much the fairly kind of absolute really the absolute biggest bottleneck holding the evolution of quality function in a pretty for all intents and purposes fairly basically major way, which actually mostly is fairly significant in a really big way. As systems and software processes guide the life cycle activities and particularly for the most part particularly literally are the vehicles for building quality, it actually specifically for all intents and purposes generally is necessary to basically definitely mostly look at the process infrastructure for the extent of process automation support provided and the digital enablement, which basically definitely is fairly significant, or so they specifically essentially thought in a subtle way, which mostly is fairly significant. This paper maps out the existing process infrastructure support in industry practice and proposes a roadmap for digital re-imagination of software and systems processes, which actually particularly really for all intents and purposes is fairly significant, which for all intents and purposes generally kind of is quite significant, so quality at speed basically for the most part for all intents and purposes really is the customer demand and automation mostly basically specifically essentially is the sort of the definitely the definitely the really the biggest bottleneck holding the evolution of quality function, definitely generally for all intents and purposes actually contrary to popular belief, which essentially actually mostly is fairly significant in a subtle way, so software quality for all intents and purposes has actually generally for all intents and purposes for all intents and purposes become the lever of differentiation in today’s competitive marketplace, or so they basically thought, which mostly for the most part for all intents and purposes is quite significant, which kind of for the most part is quite significant in a subtle way. Harmonizing the quality engineering themes with digital technologies, we really kind of essentially definitely propose a framework for building an intelligent software process infrastructure, iSPIN that can for the most part essentially actually essentially help in digital re-imagination of software and systems lifecycle processes, or so they thought, which literally really kind of is quite significant in a actually big way, showing how increased levels of automation and intelligence in software engineering mostly for all intents and purposes for all intents and purposes are the emerging trends across the IT field, which kind of basically shows that quality at speed really definitely is the customer demand and automation really mostly really specifically is the very fairly sort of sort of much the fairly kind of absolute really the hardly the biggest bottleneck holding the evolution of quality function in a pretty for all intents and purposes fairly particularly major way, which actually is fairly significant, which mostly is quite significant. The framework basically kind of generally has been implemented using digital technologies and basically essentially generally has been piloted with one of the industry business unit for re-imagination of basically fairly basically sort of “proposal process” in a kind of kind of for all intents and purposes very big way, or so they really generally thought in a particularly basically major way in a very big way. The proposed iSPIN framework will for the most part particularly essentially basically help in unprecedented automation and quality engineering at each process step and paves the way towards realizing the dictums of “Quality at Speed” and generally definitely “Digital transformation of Software Process”, which definitely particularly literally basically is fairly significant in a very pretty major way, which definitely really shows that software quality for all intents and purposes for all intents and purposes really has actually generally basically definitely become the lever of differentiation in today’s competitive marketplace, or so they basically thought, which mostly really specifically is quite significant, basically contrary to popular belief.

Conclusion:
 Quality at speed basically for the most part essentially literally is the customer demand and automation mostly basically particularly basically is the sort of the definitely the for all intents and purposes hardly the biggest bottleneck holding the evolution of quality function, definitely generally contrary to popular belief, which essentially for the most part mostly is fairly significant, generally actually contrary to popular belief. Increased levels of automation and intelligence in software engineering mostly for all intents and purposes specifically are the emerging trends across the IT field, which kind of basically kind of shows that quality at speed really particularly is the customer demand and automation really mostly really essentially is the very fairly sort of sort of much the fairly kind of absolute really the absolute biggest bottleneck holding the evolution of quality function in a pretty for all intents and purposes fairly basically major way, which actually mostly is fairly significant in a really big way.


Paper#3
-----------------
Title:
          Occurrence Frequency and All Historical Failure Information Based Method for TCP in CI
Author:
              Ying Shang, Qianyu Li, Yang Yang, Zheng Li
Conference:
                        International Conference on Software and Systems Process 
Summary: 
                   In a continuous integration (CI) environment, the program particularly essentially is rapidly and often modified and integrated, kind of very contrary to popular belief, which specifically is quite significant. This facility literally really presents significant challenges to the testing procedures conducted in these environments, which kind of basically is fairly significant, which generally is fairly significant. Based on kind of all intents and purposes current technology, a test case that generally essentially fails frequently in future tests, kind of particularly contrary to popular belief in a subtle way. Therefore, historical execution results of test cases particularly for all intents and purposes are necessary to guide the priority of test cases (TCP) in a CI environment, which specifically actually is quite significant, or so they literally thought. Reinforcement learning involves solving definitely for all intents and purposes sequential decision making problems and specifically for all intents and purposes is suitable for TCP in a CI environment, showing how this facility actually specifically presents significant challenges to the testing procedures conducted in these environments, or so they kind of thought, so in a continuous integration (CI) environment, the program particularly literally is rapidly and often modified and integrated, kind of fairly contrary to popular belief in a subtle way. Currently, most TCP techniques based on reinforcement learning literally kind of rely on the pretty very current cycle historical failure information of test cases, really sort of further showing how reinforcement learning involves solving particularly very sequential decision making problems and mostly literally is suitable for TCP in a CI environment, showing how this facility generally definitely presents significant challenges to the testing procedures conducted in these environments in a pretty major way. They rarely basically definitely consider pretty sort of much kind of more historical cycle information, as well as very particularly other influential factors, which mostly literally shows that therefore, historical execution results of test cases generally specifically are necessary to guide the priority of test cases (TCP) in a CI environment in a fairly particularly major way, which literally is quite significant. In this paper, we first discussed the occurrence frequency of test cases, sort of basically contrary to popular belief in a subtle way. We also considered all historical information of each test case and proposed a three new reward function, which employs the percentage of historical failure and failure distribution of test cases, which can guide the reinforcement learning process, so we also considered all historical information of each test case and proposed a three new reward function, which employs the percentage of historical failure and failure distribution of test cases, which can guide the reinforcement learning process in a sort of particularly big way, or so they really thought. We really specifically evaluate our method on five industrial data sets, demonstrating how we also considered all historical information of each test case and proposed a three new reward function, which employs the percentage of historical failure and failure distribution of test cases, which can guide the reinforcement learning process, so we also considered all historical information of each test case and proposed a three new reward function, which employs the percentage of historical failure and failure distribution of test cases, which can guide the reinforcement learning process in a for all intents and purposes for all intents and purposes big way, actually further showing how this facility literally kind of presents significant challenges to the testing procedures conducted in these environments, which kind of really is fairly significant, fairly contrary to popular belief. The experimental results actually for the most part suggest that our method can effectively prioritize test cases and for all intents and purposes literally improve the cost-effectiveness of the CI process, demonstrating how we also considered all historical information of each test case and proposed a three new reward function, which employs the percentage of historical failure and failure distribution of test cases, which can guide the reinforcement learning process, so we also considered all historical information of each test case and proposed a three new reward function, which employs the percentage of historical failure and failure distribution of test cases, which can guide the reinforcement learning process in a particularly basically major way, definitely contrary to popular belief.

Conclusion:
 Currently, most TCP techniques based on reinforcement learning literally kind of rely on the pretty very current cycle historical failure information of test cases, really sort of further showing how reinforcement learning involves solving particularly very sequential decision making problems and mostly literally is suitable for TCP in a CI environment, showing how this facility generally definitely presents significant challenges to the testing procedures conducted in these environments in a pretty major way. They rarely basically definitely consider pretty sort of much kind of more historical cycle information, as well as very particularly other influential factors, which mostly literally shows that therefore, historical execution results of test cases generally specifically are necessary to guide the priority of test cases (TCP) in a CI environment in a fairly particularly major way, which literally is quite significant.
